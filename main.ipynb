{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giOZe85lqHLW"
      },
      "outputs": [],
      "source": [
        "# Install and import dependencies, and set environment variables.\n",
        "\n",
        "%pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import gc\n",
        "import transformers\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wcIu-184qHLZ"
      },
      "outputs": [],
      "source": [
        "# Load and split the dataset.\n",
        "\n",
        "def load_custom_dataset():\n",
        "    dataset = load_dataset('json', data_files='all_hand_labeled_split.json')\n",
        "\n",
        "    def filter_train_split(example): return example['split'] == 'train'\n",
        "    def filter_val_split  (example): return example['split'] == 'val'\n",
        "    def filter_test_split (example): return example['split'] == 'test'\n",
        "\n",
        "    dataset = {\n",
        "        'train': dataset.filter(filter_train_split),\n",
        "        'val':   dataset.filter(filter_val_split),\n",
        "        'test':  dataset.filter(filter_test_split)\n",
        "    }\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset = load_custom_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSLDKUbTi2bq"
      },
      "outputs": [],
      "source": [
        "# Debugging tool: reduce the dataset size.\n",
        "\n",
        "# n = 1\n",
        "\n",
        "# reduced_dataset = {}\n",
        "# for split_name, dataset_dict in dataset.items():\n",
        "#     if split_name == 'train':\n",
        "#       reduced_data = dataset_dict['train']\n",
        "#     else:\n",
        "#       reduced_data = dataset_dict['train'].select(range(n))\n",
        "\n",
        "#     reduced_dataset[split_name] = DatasetDict({'train': reduced_data})\n",
        "\n",
        "# dataset = reduced_dataset\n",
        "\n",
        "# print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ffWxT_p1oVRZ"
      },
      "outputs": [],
      "source": [
        "# Define string-matching methods to extract prompts and classifications from texts.\n",
        "\n",
        "def convert_label(text):\n",
        "    if   text[-11:].strip().lower().replace('.', '') == \"compliance\": return 'complied'\n",
        "    elif text[-10:].strip().lower().replace('.', '') == \"rejection\":  return 'rejected'\n",
        "\n",
        "def extract_prompt(text):\n",
        "    if   convert_label(text) == \"complied\": return text[:-11]\n",
        "    elif convert_label(text) == \"rejected\": return text[:-10]\n",
        "    else:\n",
        "      # print(text)\n",
        "      return 'FAILED TO EXTRACT PROMPT'\n",
        "\n",
        "def extract_classification(text):\n",
        "    if   convert_label(text) == \"complied\": return 'complied'\n",
        "    elif convert_label(text) == \"rejected\": return 'rejected'\n",
        "    else:\n",
        "      print('FAILED TO EXTRACT CLASSIFICATION BELOW')\n",
        "      print(text)\n",
        "      print(\n",
        "          'FAILED TO EXTRACT CLASSIFICATION ABOVE, COMPARATOR:',\n",
        "          text[-11:].strip().lower().replace('.', '')\n",
        "      )\n",
        "      return 'FAILED TO EXTRACT PROMPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cbSu-geMojXN"
      },
      "outputs": [],
      "source": [
        "# Define how to prompt the model.\n",
        "\n",
        "def prompt_model(model, tokenizer, prompts):\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2500)\n",
        "    with torch.no_grad():\n",
        "        # results = []\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f'Processing prompt {i+1} of {len(prompts)}...')\n",
        "            try:\n",
        "                prompts[i] = pipe(prompt)[0]['generated_text']\n",
        "                # results.append(pipe(prompt)[0]['generated_text'])\n",
        "            except:\n",
        "                prompts[i] = 'ERROR GETTING MODEL RESPONSE'\n",
        "                # results.append('ERROR GETTING MODEL RESPONSE')\n",
        "\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "    return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TIDoyROToosw"
      },
      "outputs": [],
      "source": [
        "# Define a class for tracking results for a given subset of the data.\n",
        "\n",
        "class Result:\n",
        "    def __init__(self, X):\n",
        "        self.X = X\n",
        "\n",
        "        self.responses = None\n",
        "        self.predictions = None\n",
        "        self.num_invalid_predictions = None\n",
        "        self.num_mismatches = None\n",
        "        self.accuracy = None\n",
        "\n",
        "    def calculate_performance(self, y_true, model, tokenizer):\n",
        "        self.responses = prompt_model(model, tokenizer, self.X)\n",
        "        self.predictions = [extract_classification(prompt) for prompt in self.responses]\n",
        "\n",
        "        # Filter out valid predictions\n",
        "        valid_predictions = [(pred, true, idx) for idx, (pred, true) in enumerate(zip(self.predictions, y_true)) if pred in ['complied', 'rejected']]\n",
        "        self.num_invalid_predictions = len(self.predictions) - len(valid_predictions)\n",
        "\n",
        "        # Calculate mismatches among valid predictions\n",
        "        self.num_mismatches = sum(1 for pred, true, _ in valid_predictions if pred != true)\n",
        "        self.accuracy = 1 - ((self.num_invalid_predictions + self.num_mismatches) / len(self.predictions))\n",
        "\n",
        "        if len(valid_predictions) > 0:\n",
        "            self.accuracy_excluding_invalid_responses = 1 - (self.num_mismatches / len(valid_predictions))\n",
        "        else:\n",
        "            self.accuracy_excluding_invalid_responses = 1\n",
        "\n",
        "        # Calculate error rates for 'complied' and 'rejected', excluding invalid predictions\n",
        "        num_complied_true = sum(1 for _, true, _ in valid_predictions if true == 'complied')\n",
        "        num_rejected_true = sum(1 for _, true, _ in valid_predictions if true == 'rejected')\n",
        "        errors_complied = sum(1 for pred, true, _ in valid_predictions if true == 'complied' and pred != true)\n",
        "        errors_rejected = sum(1 for pred, true, _ in valid_predictions if true == 'rejected' and pred != true)\n",
        "\n",
        "        self.accuracy_complied = 1 - (errors_complied / num_complied_true if num_complied_true > 0 else 0)\n",
        "        self.accuracy_rejected = 1 - (errors_rejected / num_rejected_true if num_rejected_true > 0 else 0)\n",
        "\n",
        "        wrong_rejections = [self.X[idx] for pred, true, idx in valid_predictions if true == 'rejected' and pred != true]\n",
        "        print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>\\nExamples of responses that are labeled 'rejected' but the model predicted 'complied':\")\n",
        "        for wr in wrong_rejections[:2]:\n",
        "            print(wr)\n",
        "            print('--------------------------------------')\n",
        "        print()\n",
        "\n",
        "        wrong_compliances = [self.X[idx] for pred, true, idx in valid_predictions if true == 'complied' and pred != true]\n",
        "        print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>\\nExamples of responses that are labeled 'complied' but the model predicted 'rejected':\")\n",
        "        for wr in wrong_compliances[:2]:\n",
        "            print(wr)\n",
        "            print('--------------------------------------')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mrT6BBEqoyi8"
      },
      "outputs": [],
      "source": [
        "# Define a class for managing the train/val/test results.\n",
        "\n",
        "class SplitManager:\n",
        "    def __init__(self, split, dataset):\n",
        "        self.split = split\n",
        "\n",
        "        self.y = [item[\"tone\"] for item in dataset[split]['train']]\n",
        "\n",
        "        self.results = {\n",
        "            'zero-shot':  Result([item[\"zero_shot_instruction\"] for item in dataset[split]['train']]),\n",
        "            'few-shot':   Result([item[\"few_shot_instruction\"]  for item in dataset[split]['train']]),\n",
        "            'CoT':        Result([item[\"CoT_instruction\"]       for item in dataset[split]['train']]),\n",
        "            'finetuning': Result([item[\"zero_shot_instruction\"] for item in dataset[split]['train']])\n",
        "        }\n",
        "\n",
        "    def print_result(self, result, shot, is_finetuned):\n",
        "        model_description = \"BASE\" if not is_finetuned else \"FINE-TUNED\"\n",
        "\n",
        "        print(f'{self.split} SET, {model_description} MODEL, {shot}')\n",
        "        print(f\"% invalid items:                      {result.num_invalid_predictions/len(self.y)}\")\n",
        "        print(f\"% non-matching items:                 {result.num_mismatches/len(self.y)}\")\n",
        "        print(f\"Accuracy including invalid responses: {result.accuracy}\")\n",
        "        print(f\"Accuracy on compliances:              {result.accuracy_complied}\")\n",
        "        print(f\"Accuracy on rejections:               {result.accuracy_rejected}\")\n",
        "        print(f\"Overall accuracy:                     {result.accuracy_excluding_invalid_responses}\")\n",
        "\n",
        "    def calculate_performance_for_prompting_method(self, method, model, tokenizer, is_finetuned):\n",
        "        self.results[method].calculate_performance(self.y, model, tokenizer)\n",
        "        self.print_result(self.results[method], method, is_finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iMZ7QQGrqHLZ"
      },
      "outputs": [],
      "source": [
        "# Define how to \"replenish\" the dataset (as prompts are overwritten in place by their responses).\n",
        "\n",
        "def replenish_dataset():\n",
        "    dataset = load_custom_dataset()\n",
        "    test_split_manager = SplitManager('test', dataset)\n",
        "\n",
        "test_split_manager = SplitManager('test', dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whk3k9_xqHLZ"
      },
      "outputs": [],
      "source": [
        "# Load the base model and its tokenizer.\n",
        "\n",
        "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# 4-bit quantization configuration.\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False)\n",
        "\n",
        "# Load the model with 4-bit precision.\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=quant_config, device_map={\"\": 0})\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load the tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6JwnyA_ArlIH"
      },
      "outputs": [],
      "source": [
        "# Define a method for executing a given prompting method.\n",
        "\n",
        "def execute_prompting_method(method, is_finetuned):\n",
        "    replenish_dataset()\n",
        "    gc.collect()\n",
        "\n",
        "    for split_manager in [test_split_manager]:\n",
        "      print('\\n========================================================\\n')\n",
        "      split_manager.calculate_performance_for_prompting_method(method, model, tokenizer, is_finetuned)\n",
        "\n",
        "      for prompt in split_manager.results[method].responses:\n",
        "          if extract_classification(prompt) not in ['complied', 'rejected']:\n",
        "              # print(prompt, '\\n\\n\\n')\n",
        "              pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn20KeL0qHLa"
      },
      "outputs": [],
      "source": [
        "# Get zero-shot performance for the non-finetuned model.\n",
        "\n",
        "execute_prompting_method('zero-shot', is_finetuned=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-ZWERVdhFAK"
      },
      "outputs": [],
      "source": [
        "# Get few-shot performance for the non-finetuned model.\n",
        "\n",
        "execute_prompting_method('few-shot', is_finetuned=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaOihCXmuFIF"
      },
      "outputs": [],
      "source": [
        "# Get chain-of-thought performance for the non-finetuned model.\n",
        "\n",
        "execute_prompting_method('CoT', is_finetuned=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5d1ExUpO6S7u"
      },
      "outputs": [],
      "source": [
        "# Force clean the PyTorch cache and load training data.\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT2qSzXDqHLa"
      },
      "outputs": [],
      "source": [
        "# Configure training details.\n",
        "\n",
        "# PEFT: Parameter Effecient Fine-Tuning\n",
        "peft_params = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=50,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset['train']['train'],\n",
        "    peft_config=peft_params,\n",
        "    dataset_text_field=\"zero_shot_instruction\",\n",
        "    max_seq_length=2048,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qTo-YCM9_yaE"
      },
      "outputs": [],
      "source": [
        "# Force clean the PyTorch cache.\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX07rnJFqHLb"
      },
      "outputs": [],
      "source": [
        "# Train the model.\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf1Li1kclAUN"
      },
      "outputs": [],
      "source": [
        "# Get zero-shot performance for the fine-tuned model.\n",
        "\n",
        "execute_prompting_method('zero-shot', is_finetuned=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovwEiUp4t8N-"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "log_dir = \"results/runs\"\n",
        "notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}